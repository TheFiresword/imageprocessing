{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573422ea",
   "metadata": {},
   "source": [
    "# Active Image Alignment\n",
    "For most use cases, each band of a multispectral capture must be aligned with the other bands in order to create meaningful data. In this tutorial, we show how to align the band to each other using open source OpenCV utilities.\n",
    "\n",
    "Image alignment allows the combination of images into true-color (RGB) and false color (such as CIR) composites, useful for scouting using single images as well as for display and management uses. In addition to composite images, alignment allows the calculation of pixel-accurate indices such as NDVI or NDRE at the single image level which can be very useful for applications like plant counting and coverage estimations, where mosaicing artifacts may otherwise skew analysis results.\n",
    "\n",
    "The image alignment method described below tends to work well on images with abundant image features, or areas of significant contrast. Cars, buildings, parking lots, and roads tend to provide the best results. This approach may not work well on images which contain few features or very repetitive features, such as full canopy row crops or fields of repetitive small crops such lettuce or strawberries. We will disscuss more about the advantages and disadvantages of these methods below.\n",
    "\n",
    "The functions behind this alignment process can work with most versions of RedEdge and Altum firmware. They will work best with RedEdge (3,M,MX) versions above 3.2.0 which include the \"RigRelatives\" tags, and all RedEdge-P/Altum/Altum-PT imagery. These tags provide a starting point for the image transformation and can help to ensure convergence of the algorithm.\n",
    "\n",
    "# Opening Images\n",
    "As we have done in previous examples, we use the micasense.capture class to open, radiometrically correct, and visualize all the bands of a MicaSense capture.\n",
    "\n",
    "First, we'll load the `autoreload` extension. This lets us change underlying code (such as library functions) without having to reload the entire workbook and kernel. This is useful in this workbook because the cell that runs the alignment can take a long time to run, so with the `autoreload` extension we can update the code after the alignment step for analysis and visualization without needing to re-compute the alignments each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b09cd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import micasense.capture as capture\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.facecolor\"] = \"w\"\n",
    "\n",
    "panelNames = None\n",
    "\n",
    "# set your image paths here. See more here: https://docs.python.org/3/library/pathlib.html\n",
    "# if using Windows, you need to an an \"r\" to the path like this: Path(r\"C:\\Files\") \n",
    "\n",
    "# imagePath = Path(\"./data/REDEDGE-MX-DUAL\")\n",
    "\n",
    "# # these will return lists of image paths as strings \n",
    "# imageNames = list(imagePath.glob('IMG_0431_*.tif'))\n",
    "# imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "# panelNames = list(imagePath.glob('IMG_0000_*.tif'))\n",
    "# panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "imagePath = Path(\"./data/0000SET/000\")\n",
    "imageNames = list(imagePath.glob('IMG_0001_*.tif'))\n",
    "print(imageNames)\n",
    "imageNames = [x.as_posix() for x in imageNames]\n",
    "print(imageNames)\n",
    "#panelNames = list(imagePath.glob('IMG_0001_*.tif'))\n",
    "#panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "# imagePath = Path(\"./data/ALTUM\")\n",
    "\n",
    "# # these will return lists of image paths as strings \n",
    "# imageNames = list(imagePath.glob('IMG_0021_*.tif'))\n",
    "# imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "# panelNames = list(imagePath.glob('IMG_0000_*.tif'))\n",
    "# panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "# imagePath = Path(\"./data/REDEDGE-P\")\n",
    "\n",
    "# # these will return lists of image paths as strings \n",
    "# imageNames = list(imagePath.glob('IMG_0011_*.tif'))\n",
    "# imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "# panelNames = list(imagePath.glob('IMG_0000_*.tif'))\n",
    "# panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "#imagePath = Path(\"./data/ALTUM-PT\")\n",
    "\n",
    "# these will return lists of image paths as strings \n",
    "#imageNames = list(imagePath.glob('IMG_0010_*.tif'))\n",
    "#imageNames = [x.as_posix() for x in imageNames]\n",
    "\n",
    "#panelNames = list(imagePath.glob('IMG_0000_*.tif'))\n",
    "#panelNames = [x.as_posix() for x in panelNames]\n",
    "\n",
    "\n",
    "if panelNames is not None:\n",
    "    panelCap = capture.Capture.from_filelist(panelNames)\n",
    "else:\n",
    "    panelCap = None\n",
    "\n",
    "thecapture = capture.Capture.from_filelist(imageNames)\n",
    "\n",
    "# get camera model for future use \n",
    "cam_model = thecapture.camera_model\n",
    "# if this is a multicamera system like the RedEdge-MX Dual,\n",
    "# we can combine the two serial numbers to help identify \n",
    "# this camera system later. \n",
    "if len(thecapture.camera_serials) > 1:\n",
    "    cam_serial = \"_\".join(thecapture.camera_serials)\n",
    "    print(cam_serial)\n",
    "else:\n",
    "    cam_serial = thecapture.camera_serial\n",
    "    \n",
    "print(\"Camera model:\",cam_model)\n",
    "print(\"Bit depth:\", thecapture.bits_per_pixel)\n",
    "print(\"Camera serial number:\", cam_serial)\n",
    "print(\"Capture ID:\",thecapture.uuid)\n",
    "\n",
    "# determine if this sensor has a panchromatic band \n",
    "if cam_model == 'RedEdge-P' or cam_model == 'Altum-PT':\n",
    "    panchroCam = True\n",
    "else:\n",
    "    panchroCam = False\n",
    "    panSharpen = False \n",
    "\n",
    "if panelCap is not None:\n",
    "    if panelCap.panel_albedo() is not None:\n",
    "        panel_reflectance_by_band = panelCap.panel_albedo()\n",
    "    else:\n",
    "        panel_reflectance_by_band = [0.49]*len(thecapture.eo_band_names()) #RedEdge band_index order\n",
    "    panel_irradiance = panelCap.panel_irradiance(panel_reflectance_by_band)  \n",
    "    irradiance_list = panelCap.panel_irradiance(panel_reflectance_by_band) + [0] # add to account for uncalibrated LWIR band, if applicable\n",
    "    img_type = \"reflectance\"\n",
    "    thecapture.plot_undistorted_reflectance(panel_irradiance)\n",
    "else:\n",
    "    if thecapture.dls_present():\n",
    "        img_type='reflectance'\n",
    "        irradiance_list = thecapture.dls_irradiance() + [0]\n",
    "        thecapture.plot_undistorted_reflectance(thecapture.dls_irradiance())\n",
    "    else:\n",
    "        img_type = \"radiance\"\n",
    "        thecapture.plot_undistorted_radiance() \n",
    "        irradiance_list = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5a774",
   "metadata": {},
   "source": [
    "# Check for existing warp matrices \n",
    "If we have already successfully aligned captures from this specific camera, we can typically save some time and use the alignment warp matrices for other captures from the same camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35de1be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing warp matrices found. Create them later in the notebook.\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import ProjectiveTransform\n",
    "import numpy as np\n",
    "\n",
    "if panchroCam:\n",
    "    warp_matrices_filename = str(cam_serial) + \"_warp_matrices_SIFT.npy\"\n",
    "else:\n",
    "    warp_matrices_filename = str(cam_serial) + \"_warp_matrices_opencv.npy\"\n",
    "\n",
    "if Path('./' + warp_matrices_filename).is_file():\n",
    "    print(\"Found existing warp matrices for camera\", cam_serial)\n",
    "    load_warp_matrices = np.load(warp_matrices_filename, allow_pickle=True)\n",
    "    loaded_warp_matrices = []\n",
    "    for matrix in load_warp_matrices: \n",
    "        if panchroCam:\n",
    "            transform = ProjectiveTransform(matrix=matrix.astype('float64'))\n",
    "            loaded_warp_matrices.append(transform)\n",
    "        else:\n",
    "            loaded_warp_matrices.append(matrix.astype('float32'))\n",
    "    print(\"Warp matrices successfully loaded.\")\n",
    "\n",
    "    if panchroCam:\n",
    "        warp_matrices_SIFT = loaded_warp_matrices\n",
    "    else:\n",
    "        warp_matrices = loaded_warp_matrices\n",
    "else:\n",
    "    print(\"No existing warp matrices found. Create them later in the notebook.\")\n",
    "    warp_matrices_SIFT = False\n",
    "    warp_matrices = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185627ff",
   "metadata": {},
   "source": [
    "# Unwarp and Align (OpenCV method for RedEdge3/M/MX/Dual and original Altum)\n",
    "Alignment is a three-step process:\n",
    "\n",
    "Images are unwarped using the built-in lens calibration\n",
    "A transformation is found to align each band to a common band\n",
    "The aligned images are combined and cropped, removing pixels which don't overlap in all bands.\n",
    "We provide utilities to find the alignment transformations within a single capture. Our experience shows that once a good alignment transformation is found, it tends to be stable over a flight and, in most cases, over many flights. The transformation may change if the camera undergoes a shock event (such as a hard landing or drop) or if the temperature changes substantially between flights. In these events, a new transformation may need to be found.\n",
    "\n",
    "Further, since this approach finds a 2-dimensional (affine) transformation between images, it won't work when the parallax between bands results in a 3-dimensional depth field. This can happen if very close to the target or when targets are visible at significantly different ranges, such as a nearby tree or building against a background much farther way. In these cases, it will be necessary to use photogrammetry techniques to find a 3-dimensional mapping between images.\n",
    "\n",
    "For best alignment results it's good to select a capture which has features which visible in all bands. Man-made objects such as cars, roads, and buildings tend to work very well, while captures of only repeating crop rows tend to work poorly. Remember, once a good transformation has been found for flight, it can be generally be applied across all of the images.\n",
    "\n",
    "It's also good to use an image for alignment which is taken near the same level above ground as the rest of the flights. Above approximately 35m AGL, the alignment will be consistent. However, if images taken closer to the ground are used, such as panel images, the same alignment transformation will not work for the flight data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import micasense.imageutils as imageutils\n",
    "import micasense.plotutils as plotutils\n",
    "\n",
    "# We use a different alignment method for RedEdge-P and Altum-PT, \n",
    "# so if the imagery is from this kind of camera, skip this step \n",
    "if not panchroCam:\n",
    "    st = time.time()\n",
    "    # set to True if you'd like to ignore existing warp matrices and create new ones\n",
    "    regenerate = True    \n",
    "    pyramid_levels = 0 # for images with RigRelatives, setting this to 0 or 1 may improve alignment\n",
    "    max_alignment_iterations = 10\n",
    "\n",
    "    # match_index: \n",
    "    # for non-panchromatic cameras we want to use band 1, which is green \n",
    "    # the green band has zero rig relative offsets \n",
    "    # NOTE: These band numbers are zero-indexed \n",
    "    # special parameters for RedEdge-MX dual camera system \n",
    "    if len(thecapture.eo_band_names()) == 10:\n",
    "        print(\"is 10 band\")\n",
    "        pyramid_levels = 3\n",
    "        match_index = 4\n",
    "        max_alignment_iterations = 20\n",
    "    else: \n",
    "        match_index = 1\n",
    "\n",
    "    warp_mode = cv2.MOTION_HOMOGRAPHY # MOTION_HOMOGRAPHY or MOTION_AFFINE. For Altum images only use HOMOGRAPHY\n",
    "    \n",
    "    print(\"Aligning images. Depending on settings this can take from a few seconds to many minutes\")\n",
    "    # Can potentially increase max_iterations for better results, but longer runtimes\n",
    "    if warp_matrices and not regenerate:\n",
    "        print(\"Using existing warp matrices...\")\n",
    "        try:\n",
    "            irradiance = panel_irradiance+[0]\n",
    "        except NameError:\n",
    "            irradiance = None\n",
    "    else:\n",
    "        warp_matrices, alignment_pairs = imageutils.align_capture(thecapture,\n",
    "                                                              ref_index = match_index,\n",
    "                                                              max_iterations = max_alignment_iterations,\n",
    "                                                              warp_mode = warp_mode,\n",
    "                                                              pyramid_levels = pyramid_levels)\n",
    "\n",
    "    print(\"Finished Aligning\")\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "    print('Alignment time:', int(elapsed_time), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bfdf3",
   "metadata": {},
   "source": [
    "# Crop Aligned Images and create aligned capture stack \n",
    "After finding image alignments, we may need to remove pixels around the edges which aren't present in every image in the capture. To do this we use the affine transforms found above and the image distortions from the image metadata. OpenCV provides a couple of handy helpers for this task in the cv2.undistortPoints() and cv2.transform() methods. These methods take a set of pixel coordinates and apply our undistortion matrix and our affine transform, respectively. So, just as we did when registering the images, we first apply the undistortion process to the coordinates of the image borders, then we apply the affine transformation to that result. The resulting pixel coordinates tell us where the image borders end up after this pair of transformations, and we can then crop the resultant image to these coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not panchroCam:\n",
    "    cropped_dimensions, edges = imageutils.find_crop_bounds(thecapture, warp_matrices, warp_mode=warp_mode, reference_band=match_index)\n",
    "    print(\"Cropped dimensions:\",cropped_dimensions)\n",
    "    im_aligned = thecapture.create_aligned_capture(warp_matrices=warp_matrices, motion_type=warp_mode, img_type=img_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d51b1",
   "metadata": {},
   "source": [
    "# Save warp matrices\n",
    "Once an alignment for your camera has been found, it can be saved to a file for later use with this notebook. It can also be used on the Batch Alignment notebook for aligning all of the captures from an entire flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84855db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import skimage\n",
    "from skimage.transform import warp,matrix_transform,resize,FundamentalMatrixTransform,estimate_transform,ProjectiveTransform\n",
    "\n",
    "if panchroCam:\n",
    "    working_wm = warp_matrices_SIFT\n",
    "else:\n",
    "    working_wm = warp_matrices\n",
    "if not Path('./' + warp_matrices_filename).is_file() or regenerate:\n",
    "    temp_matrices = []\n",
    "    for x in working_wm:\n",
    "        if isinstance(x, numpy.ndarray):\n",
    "            temp_matrices.append(x)\n",
    "        if isinstance(x, skimage.transform._geometric.ProjectiveTransform):\n",
    "            temp_matrices.append(x.params)\n",
    "    np.save(warp_matrices_filename, np.array(temp_matrices, dtype=object), allow_pickle=True)\n",
    "    print(\"Saved to\", Path('./' + warp_matrices_filename).resolve())\n",
    "else:\n",
    "    print(\"Matrices already exist at\",Path('./' + warp_matrices_filename).resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb819b",
   "metadata": {},
   "source": [
    "# Visualize Aligned Images\n",
    "Once the transformation has been found, it can be verified by compositing the aligned images to check alignment. The image 'stack' containing all bands can also be exported to a multi-band TIFF file for viewing in external software such as QGIS. Useful composites are a naturally colored RGB as well as color infrared, or CIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8626c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize=(30,23) # use this size for full-image-resolution display\n",
    "figsize=(16,13)   # use this size for export-sized display\n",
    "\n",
    "rgb_band_indices = [thecapture.band_names_lower().index('red'),\n",
    "                    thecapture.band_names_lower().index('green'),\n",
    "                    thecapture.band_names_lower().index('blue')]\n",
    "cir_band_indices = [thecapture.band_names_lower().index('nir'),\n",
    "                    thecapture.band_names_lower().index('red'),\n",
    "                    thecapture.band_names_lower().index('green')]\n",
    "\n",
    "# Create normalized stacks for viewing\n",
    "im_display = np.zeros((im_aligned.shape[0],im_aligned.shape[1],im_aligned.shape[2]), dtype=np.float32)\n",
    "im_min = np.percentile(im_aligned[:,:,rgb_band_indices].flatten(), 0.5)  # modify these percentiles to adjust contrast\n",
    "im_max = np.percentile(im_aligned[:,:,rgb_band_indices].flatten(), 99.5)  # for many images, 0.5 and 99.5 are good values\n",
    "\n",
    "if panchroCam:\n",
    "    im_display_sharp = np.zeros((sharpened_stack.shape[0],sharpened_stack.shape[1],sharpened_stack.shape[2]), dtype=np.float32 )\n",
    "    im_min_sharp = np.percentile(sharpened_stack[:,:,rgb_band_indices].flatten(), 0.5)  # modify these percentiles to adjust contrast\n",
    "    im_max_sharp = np.percentile(sharpened_stack[:,:,rgb_band_indices].flatten(), 99.5)  # for many images, 0.5 and 99.5 are good values\n",
    "\n",
    "\n",
    "# for rgb true color, we use the same min and max scaling across the 3 bands to \n",
    "# maintain the \"white balance\" of the calibrated image\n",
    "for i in rgb_band_indices:\n",
    "    im_display[:,:,i] =  imageutils.normalize(im_aligned[:,:,i], im_min, im_max)\n",
    "    if panchroCam: \n",
    "        im_display_sharp[:,:,i] = imageutils.normalize(sharpened_stack[:,:,i], im_min_sharp, im_max_sharp)\n",
    "\n",
    "rgb = im_display[:,:,rgb_band_indices]\n",
    "\n",
    "if panchroCam:\n",
    "    rgb_sharp = im_display_sharp[:,:,rgb_band_indices]\n",
    "\n",
    "nir_band = thecapture.band_names_lower().index('nir')\n",
    "red_band = thecapture.band_names_lower().index('red')\n",
    "\n",
    "ndvi = (im_aligned[:,:,nir_band] - im_aligned[:,:,red_band]) / (im_aligned[:,:,nir_band] + im_aligned[:,:,red_band])\n",
    "\n",
    "# for cir false color imagery, we normalize the NIR,R,G bands within themselves, which provides\n",
    "# the classical CIR rendering where plants are red and soil takes on a blue tint\n",
    "for i in cir_band_indices:\n",
    "    im_display[:,:,i] =  imageutils.normalize(im_aligned[:,:,i])\n",
    "\n",
    "cir = im_display[:,:,cir_band_indices]\n",
    "if panchroCam:\n",
    "    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "else:\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=figsize)\n",
    "ax1.set_title(\"Red-Green-Blue Composite\")\n",
    "ax1.imshow(rgb)\n",
    "if panchroCam:\n",
    "    ax2.set_title(\"Red-Green-Blue Composite (pan-sharpened)\")\n",
    "    ax2.imshow(rgb_sharp)\n",
    "\n",
    "fig, (ax3,ax4) = plt.subplots(1, 2, figsize=figsize)\n",
    "ax3.set_title(\"NDVI\")\n",
    "ax3.imshow(ndvi)\n",
    "ax4.set_title(\"Color Infrared (CIR) Composite\")\n",
    "ax4.imshow(cir)\n",
    "\n",
    "# set custom lims if you want to zoom in to image to see more detail \n",
    "# this is useful for comparing upsampled and pan-sharpened stacks \n",
    "# custom_xlim=(1500,2000)\n",
    "# custom_ylim=(2000,1500)\n",
    "# plt.setp([ax1,ax2,ax3,ax4], xlim=custom_xlim, ylim=custom_ylim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f6189",
   "metadata": {},
   "source": [
    "# Stack Export\n",
    "We can easily export the stacks into an image using the GDAL library (http://www.glal.org). Once exported, these image stacks can be opened in software such as QGIS and raster operations such as NDVI or NDRE computation can be done in that software. The stacks include geographic information. \n",
    "\n",
    "If you prefer, you may set `sort_by_wavelength` to `True` in the `save_capture_as_stack` function.\n",
    "\n",
    "Unless otherwise specified, this will save in your working folder, that is your `imageprocessing` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d716be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set output name to unique capture ID, e.g. FWoNSvgDNBX63Xv378qs\n",
    "outputName = thecapture.uuid\n",
    "\n",
    "st = time.time()\n",
    "if panchroCam:\n",
    "    # in this example, we can export both a pan-sharpened stack and an upsampled stack\n",
    "    # so you can compare them in GIS. In practice, you would typically only output the pansharpened stack \n",
    "    thecapture.save_capture_as_stack(outputName+\"-pansharpened.tif\", sort_by_wavelength=True, pansharpen=True)\n",
    "    thecapture.save_capture_as_stack(outputName+\"-upsampled.tif\", sort_by_wavelength=True, pansharpen=False)\n",
    "else:\n",
    "    thecapture.save_capture_as_stack(outputName+\"-noPanels.tif\", sort_by_wavelength=True)\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print(\"Time to save stacks:\", int(elapsed_time), \"seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
